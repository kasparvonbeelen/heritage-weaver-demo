{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31153, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prepare data for export and training in Colab\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/heritage_weaver_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data/siglip-training: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/siglip-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!mkdir data/siglip-training/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30130"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloaded_images = list(df[df['img_path'] != ''].img_path.unique())\n",
    "len(downloaded_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['smg_imgs/204|255|medium_cd0620_049_100527_2005_86_35_Professional_audio_cassette_tape_used_by_Radio_Manchester.jpg',\n",
       "  'smg_imgs/477|975|medium_SMG00247371.jpg',\n",
       "  'smg_imgs/58|255|medium_1982_1712__0001_.jpg',\n",
       "  'smg_imgs/58|256|medium_1982_1712__0002_.jpg',\n",
       "  'smg_imgs/58|257|medium_1982_1712__0003_.jpg',\n",
       "  'smg_imgs/212|509|medium_cd0404_011_080808_2002_19_100_LCM_Speed_recorder_opened.jpg',\n",
       "  'smg_imgs/212|510|medium_cd0404_012_080808_2002_19_100_LCM_Speed_recorder.jpg',\n",
       "  'smg_imgs/247|329|medium_cd0098_006_050329_GG_1991_126_11_Light_shade.jpg',\n",
       "  'smg_imgs/209|376|medium_cd0472_015_081216_1996_10_507_Ferranti_9E_Radio_component.jpg',\n",
       "  'smg_imgs/247|310|medium_cd0097_009_050329_GG_1990_25_3_Gas_light.jpg'],\n",
       " 30130)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloaded_images[:10], len(downloaded_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30130/30130 [01:37<00:00, 307.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# write a for loop to copy the images to the clip-training folder\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "for img in tqdm(downloaded_images):\n",
    "    shutil.copy(img, os.path.join('data/siglip-training/images', os.path.basename(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import SiglipProcessor, SiglipModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for saving the model\n",
    "Path('models').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SigLIP model and processor\n",
    "checkpoint = \"google/siglip-base-patch16-224\"\n",
    "processor = SiglipProcessor.from_pretrained(checkpoint)\n",
    "model = SiglipModel.from_pretrained(checkpoint)\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/heritage_weaver_data.csv', index_col=0)\n",
    "df = df[['name','description','img_path']].dropna().reset_index(drop=True)\n",
    "df['filepath'] = df['img_path'].apply(lambda x: '/content/images/' + x.split('/')[-1])\n",
    "df['downloaded'] =df['filepath'].apply(lambda x: Path(x).is_file())\n",
    "df = df[df.downloaded==True].sample(frac=1.0).reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = int(len(df)*.9)\n",
    "df_train = df.iloc[:threshold]\n",
    "df_eval = df.iloc[threshold:]\n",
    "df_train[['filepath','name']].to_csv('train.csv', sep='\\t') # name | description\n",
    "df_eval[['filepath','name']].to_csv('eval.csv', sep='\\t') # name | description\n",
    "df_train.shape, df_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class image_title_dataset():\n",
    "    def __init__(self, df, column='name'):  # description | name\n",
    "        # Initialize data\n",
    "        self.df = df\n",
    "        self.column = column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess image using SigLIPS's preprocessing function\n",
    "        processed = processor(text=[self.df.iloc[idx][self.column]], \n",
    "                              images=[Image.open(self.df.iloc[idx].filepath)],\n",
    "                         return_tensors=\"pt\",\n",
    "                         max_length=64,\n",
    "                         padding='max_length', truncation=True)\n",
    "        return processed['input_ids'].to(device), processed['pixel_values'].squeeze(0).to(device) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = image_title_dataset(df_train)\n",
    "dataset_eval = image_title_dataset(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=32, shuffle=True) #Define your own dataloader\n",
    "eval_dataloader = DataLoader(dataset_eval, batch_size=32, shuffle=True) #Define your own dataloader\n",
    "\n",
    "# Specify the loss function\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handy utility at https://github.com/wenwei202/pytorch-examples/blob/ecbb7beb0fac13133c0b09ef980caf002969d315/imagenet/main.py#L296\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "lowest_loss = 999\n",
    "num_epochs = 10\n",
    "\n",
    "losses_train = AverageMeter()\n",
    "losses_eval = AverageMeter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        texts, images = batch\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(texts, images)\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(output.logits_per_image,ground_truth) + loss_txt(output.logits_per_text,ground_truth))/2\n",
    "        losses_train.update(total_loss.item(), len(images))\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += total_loss.item()\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "          print('Epoch: [{0}]\\t'\n",
    "                'Training Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                  epoch, loss=losses_train,))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(eval_dataloader)):\n",
    "      texts,images = batch\n",
    "      # Forward pass\n",
    "      output = model(texts, images)\n",
    "      # Compute loss\n",
    "      ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "      total_loss = (loss_img(output.logits_per_image,ground_truth) + loss_txt(output.logits_per_text,ground_truth))/2\n",
    "      losses_eval.update(total_loss.item(), len(images))\n",
    "\n",
    "      if idx % 10 == 0:\n",
    "        print('Epoch: [{0}]\\t'\n",
    "                  'Eval Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   epoch, loss=losses_eval))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'./models/{checkpoint}-ft-last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"Kaspar/clip-heritage-weaver-name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heritageweaverv",
   "language": "python",
   "name": "heritageweaverv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
