{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weavingtools.weaving_tools import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "Path('models').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class image_title_dataset():\n",
    "    def __init__(self, df):\n",
    "        # Initialize data\n",
    "        self.df = df \n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess image using CLIP's preprocessing function\n",
    "        #print(self.df.iloc[idx]['name'])\n",
    "        processed = processor(text=[self.df.iloc[idx]['name']], images=[Image.open(self.df.iloc[idx].img_path)], \n",
    "                         return_tensors=\"pt\", \n",
    "                         max_length=77, padding='max_length', truncation=True)\n",
    "        return processed['input_ids'].to(device), processed['pixel_values'].squeeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/communications_data_updated.csv')\n",
    "df = df[['name','img_path']].dropna().reset_index(drop=True)\n",
    "df['filepath'] = df['img_path'].apply(lambda x: '/content/img_data/' + x.split('/')[-1])\n",
    "df = df.sample(frac=1.0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = int(len(df)*.9)\n",
    "df_train = df.iloc[:threshold]\n",
    "df_eval = df.iloc[threshold:]\n",
    "df_train[['filepath','name']].to_csv('train.csv', sep='\\t')\n",
    "df_eval[['filepath','name']].to_csv('eval.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# imgs_path = Path('data/img_data')\n",
    "# imgs_path.mkdir(exist_ok=True)\n",
    "# for i,row in tqdm(df.iterrows(), total=len(df)):\n",
    "#     img = Image.open(row['img_path'])\n",
    "#     img.save(imgs_path / row['img_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose computation device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\" \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model and processor\n",
    "checkpoint = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(checkpoint)\n",
    "processor = CLIPProcessor.from_pretrained(checkpoint)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = image_title_dataset(df_train)\n",
    "dataset_eval = image_title_dataset(df_eval)\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=32, shuffle=True) #Define your own dataloader\n",
    "eval_dataloader = DataLoader(dataset_eval, batch_size=32, shuffle=True) #Define your own dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "# Specify the loss function\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "lowest_loss = 999\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar_train = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for batch in pbar_train:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images,texts = batch\n",
    "\n",
    "        images= images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(images, texts)\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(output.logits_per_image,ground_truth) + loss_txt(output.logits_per_text,ground_truth))/2\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        #if device == \"cpu\":\n",
    "        optimizer.step()\n",
    "        #else :\n",
    "        #    convert_models_to_fp32(model)\n",
    "        #    optimizer.step()\n",
    "        #    model.convert_weights(model)\n",
    "        running_loss += total_loss.item()\n",
    "        pbar_train.set_description()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    running_loss_eval = 0.0\n",
    "    pbar_eval = tqdm(eval_dataloader, total=len(eval_dataloader))\n",
    "    for batch in pbar_eval:\n",
    "      images,texts = batch\n",
    "      images = images.to(device)\n",
    "      texts = texts.to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      output = model(images, texts)\n",
    "      # Compute loss\n",
    "      ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "      total_loss = (loss_img(output.logits_per_image,ground_truth) + loss_txt(output.logits_per_text,ground_truth))/2\n",
    "      running_loss_eval+=total_loss.item()\n",
    "\n",
    "\n",
    "    current_loss = running_loss_eval/len(eval_dataloader)\n",
    "    if current_loss < lowest_loss:\n",
    "      model.save_pretrained(f'./models/{checkpoint}-ft')\n",
    "      lowest_loss = current_loss\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Training Loss: {running_loss/len(train_dataloader):.4f} Validation Loss {current_loss:.4f}\")\n",
    "\n",
    "model.save_pretrained(f'./models/{checkpoint}-ft-last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "#     for batch in pbar:\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         images,texts = batch \n",
    "        \n",
    "#         images= images.to(device)\n",
    "#         texts = texts.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         #logits_per_image, logits_per_text = \n",
    "#         output = model(images, texts)\n",
    "#         # Compute loss\n",
    "#         ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "#         total_loss = (loss_img(output.logits_per_image,ground_truth) + loss_txt(output.logits_per_text,ground_truth))/2\n",
    "\n",
    "#         # Backward pass\n",
    "#         total_loss.backward()\n",
    "#         #if device == \"cpu\":\n",
    "#         optimizer.step()\n",
    "#         #else : \n",
    "#         #    convert_models_to_fp32(model)\n",
    "#         #    optimizer.step()\n",
    "#         #    clip.model.convert_weights(model)\n",
    "#         running_loss += total_loss.item()\n",
    "#         pbar.set_description()\n",
    "\n",
    "    \n",
    "#     print(f\"Epoch {epoch}/{num_epochs}, Loss: {running_loss/len(train_dataloader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heritageweaverv",
   "language": "python",
   "name": "heritageweaverv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
