{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This notebook selects data to evaluate the use of HW for linking. It is mainly used for selecting image pairs for annotation.\n",
    "\n",
    "We aim to evaluate\n",
    "- multimodality (linking by image, text or both)\n",
    "- model fine-tuning (fine-tuned models vs original siglip)\n",
    "\n",
    "We ask people to annotate n image pairs\n",
    "- for each multimodal search strategy we select records pairs using the following criteria:\n",
    "    - Linked by all models\n",
    "    - Linked only by the fine-tuned models\n",
    "    - Linked only by siglip\n",
    "    - Not linked\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from weavingtools.annotation_tools import *\n",
    "from weavingtools.annotation_tools import plot_by_record, open_image\n",
    "from weavingtools.linkage_tools import *\n",
    "from weavingtools.embedding_tools import *\n",
    "import scipy.spatial as sp\n",
    "import ipyannotations.generic\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_db = load_db(\"hw\",'heritage_weaver','google/siglip-base-patch16-224')\n",
    "collection_db_ft = load_db(\"hw\",'heritage_weaver_ft','Kaspar/siglip-heritage-weaver-name')\n",
    "collection_db_ft_text = load_db(\"hw\",'heritage_weaver_ft_text','Kaspar/siglip-heritage-weaver-text')\n",
    "collection_df = pd.read_csv('data/heritage_weaver_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select image to image linking pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = 'KB'\n",
    "coll1, coll2 = 'smg','nms'\n",
    "percentile = 99.0 \n",
    "randomize = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get inputs...\n",
      "Compute similarities...\n",
      "--- Get similarities ---\n",
      "--- Using 0.759627968792046 as threshold ---\n",
      "--- Aggregate similarities by record ---\n",
      "--- Threshold similarities and binarize ---\n",
      "Retrieve edges...\n",
      "Get inputs...\n",
      "Compute similarities...\n",
      "--- Get similarities ---\n",
      "--- Using 0.8251287715322748 as threshold ---\n",
      "--- Aggregate similarities by record ---\n",
      "--- Threshold similarities and binarize ---\n",
      "Retrieve edges...\n",
      "Get inputs...\n",
      "Compute similarities...\n",
      "--- Get similarities ---\n",
      "--- Using 0.7797531082765015 as threshold ---\n",
      "--- Aggregate similarities by record ---\n",
      "--- Threshold similarities and binarize ---\n",
      "Retrieve edges...\n"
     ]
    }
   ],
   "source": [
    "edges_img_sigl, _, _ = get_edges(collection_db,coll1,coll2, 'image','image', 'max',percentile )\n",
    "edges_img_sigl_ft, _, _ = get_edges(collection_db_ft,coll1,coll2, 'image','image', 'max',percentile )\n",
    "edges_img_sigl_ft_text, _, _ = get_edges(collection_db_ft_text,coll1,coll2, 'image','image', 'max',percentile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agreement between the three methods\n",
    "img_common_links = set(edges_img_sigl).intersection(edges_img_sigl_ft,edges_img_sigl_ft_text)\n",
    "# only in siglip not an edge returned by fine-tuned models\n",
    "img_only_siglip = set(edges_img_sigl).difference(edges_img_sigl_ft,edges_img_sigl_ft_text)\n",
    "# agreement among fine-tuned models but not by siglip\n",
    "img_agreement_only_ft = set(edges_img_sigl_ft).intersection(edges_img_sigl_ft_text).difference(edges_img_sigl)\n",
    "# only siglip ft \n",
    "only_siglip_ft = set(edges_img_sigl_ft).difference(edges_img_sigl_ft_text,edges_img_sigl)\n",
    "# only siglip ft text\n",
    "oimg_nly_siglip_ft_text = set(edges_img_sigl_ft_text).difference(edges_img_sigl_ft,edges_img_sigl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get inputs...\n",
      "Compute similarities...\n",
      "--- Get similarities ---\n",
      "--- Using 0.791760886322269 as threshold ---\n",
      "--- Aggregate similarities by record ---\n",
      "--- Threshold similarities and binarize ---\n",
      "Retrieve edges...\n",
      "Get inputs...\n",
      "Compute similarities...\n",
      "--- Get similarities ---\n",
      "--- Using 0.8642581257801918 as threshold ---\n",
      "--- Aggregate similarities by record ---\n",
      "--- Threshold similarities and binarize ---\n",
      "Retrieve edges...\n",
      "Get inputs...\n",
      "Compute similarities...\n",
      "--- Get similarities ---\n",
      "--- Using 0.8795189101878229 as threshold ---\n",
      "--- Aggregate similarities by record ---\n",
      "--- Threshold similarities and binarize ---\n",
      "Retrieve edges...\n"
     ]
    }
   ],
   "source": [
    "edges_txt_sigl, _, _ = get_edges(collection_db,coll1,coll2, 'text','text', 'max',percentile )\n",
    "edges_txt_sigl_ft, _, _ = get_edges(collection_db_ft,coll1,coll2, 'text','text', 'max',percentile )\n",
    "edges_txt_sigl_ft_text, _, _ = get_edges(collection_db_ft_text,coll1,coll2, 'text','text', 'max',percentile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agreement between the three methods\n",
    "txt_common_links = set(edges_txt_sigl).intersection(edges_txt_sigl_ft,edges_txt_sigl_ft_text)\n",
    "# only in siglip not an edge returned by fine-tuned models\n",
    "txt_only_siglip = set(edges_txt_sigl).difference(edges_txt_sigl_ft,edges_txt_sigl_ft_text)\n",
    "# agreement among fine-tuned models but not by siglip\n",
    "txt_agreement_only_ft = set(edges_txt_sigl_ft).intersection(edges_txt_sigl_ft_text).difference(edges_txt_sigl)\n",
    "# only siglip ft \n",
    "txt_only_siglip_ft = set(edges_txt_sigl_ft).difference(edges_txt_sigl_ft_text,edges_txt_sigl)\n",
    "# only siglip ft text\n",
    "txt_only_siglip_ft_text = set(edges_txt_sigl_ft_text).difference(edges_txt_sigl_ft,edges_txt_sigl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agreement between the three methods\n",
    "mult_common_links = set(txt_common_links).intersection(img_common_links)\n",
    "# only in siglip not an edge returned by fine-tuned models\n",
    "mult_only_siglip = set(txt_only_siglip).intersection(img_only_siglip)\n",
    "# agreement among fine-tuned models but not by siglip\n",
    "mult_agreement_only_ft = set(txt_agreement_only_ft).intersection(img_agreement_only_ft)\n",
    "# only siglip ft \n",
    "mult_only_siglip_ft = set(txt_only_siglip_ft).intersection(only_siglip_ft)\n",
    "# only siglip ft text\n",
    "mult_only_siglip_ft_text = set(txt_only_siglip_ft_text).intersection(oimg_nly_siglip_ft_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = [\n",
    "   ('img_common_links', img_common_links), \n",
    "    ('img_only_siglip', img_only_siglip), \n",
    "    ('img_agreement_only_ft', img_agreement_only_ft), \n",
    "    ('only_siglip_ft', only_siglip_ft), \n",
    "    ('oimg_nly_siglip_ft_text', oimg_nly_siglip_ft_text),\n",
    "    ('txt_common_links', txt_common_links), \n",
    "    ('txt_only_siglip', txt_only_siglip), \n",
    "    ('txt_agreement_only_ft', txt_agreement_only_ft), \n",
    "    ('txt_only_siglip_ft', txt_only_siglip_ft), \n",
    "    ('txt_only_siglip_ft_text', txt_only_siglip_ft_text),\n",
    "    ('mult_common_links', mult_common_links), \n",
    "    ('mult_only_siglip', mult_only_siglip), \n",
    "    ('mult_agreement_only_ft', mult_agreement_only_ft), \n",
    "    ('mult_only_siglip_ft', mult_only_siglip_ft), \n",
    "    ('mult_only_siglip_ft_text', mult_only_siglip_ft_text)\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "to_annotate = []\n",
    "top_n = 10\n",
    "for name, links in all_links:\n",
    "    links = list(links)\n",
    "    random.shuffle(links)\n",
    "    to_annotate.extend([(name, *link) for link in links[:top_n]])\n",
    "\n",
    "df_annotation = pd.DataFrame(to_annotate, columns=['link_type','source','target'])\n",
    "df_annotation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 13)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imgs = collection_df[['record_id','img_url','img_path','description','name']].drop_duplicates().reset_index(drop=True)\n",
    "df_annotation_with_img = df_annotation.merge(\n",
    "    df_imgs, \n",
    "    left_on='source', right_on='record_id', how='left'\n",
    "        ).merge(df_imgs, left_on='target', right_on='record_id', how='left', suffixes=('_source','_target'))\n",
    "df_annotation_with_img.drop_duplicates(subset=['source','target'], inplace=True)\n",
    "df_annotation_with_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation_with_img.to_csv('data/heritage_weaver_annotations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_pair_image(record_description_pair):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 7.5))\n",
    "    \n",
    "    for i,pair in enumerate(record_description_pair):\n",
    "        #i+=1\n",
    "        img_path = pair[0]\n",
    "        description = soft_wrap_text(pair[1][:200])\n",
    "        \n",
    "        # try:\n",
    "        #   #img = Image.open(requests.get(img_path,  stream=True).raw,).convert('RGB')\n",
    "        # except:\n",
    "        #   try:\n",
    "        #     img_path = 'https://www.nms.ac.uk/api/axiell?command=getcontent&server=Detail&value=' + img_path.split('value=')[-1]\n",
    "        #     data = requests.get(img_path)\n",
    "        #     img = Image.open(io.BytesIO(bytes(data.content)))\n",
    "        #   except:\n",
    "        #     print(img_path)\n",
    "        #     img = Image.open('./heritageweaver/data/No_Image_Available.jpg').convert(\"RGB\")\n",
    "        #     img.thumbnail((224, 224))\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img.thumbnail((224, 224))\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(description, fontsize = 18)\n",
    "        axes[i].axis('off')\n",
    "         \n",
    "    #plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_source = list(df_annotation_with_img[['img_path_source','description_source']].values)\n",
    "image_target = list(df_annotation_with_img[['img_path_target','description_target']].values)\n",
    "image_pairs = list(zip(image_source, image_target))\n",
    "len(image_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = [[i,record_pair_image(p)] for i,p in enumerate(image_pairs[:3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(data).to_excel('data/heritage_weaver_annotations.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "def save_images_for_annoation(image_pairs, output_path):\n",
    " \n",
    "    \"\"\"\n",
    "    Save a DataFrame to Excel with matplotlib-generated images in specific cells.\n",
    "    \n",
    "    Parameters:\n",
    "    - image pairs:\n",
    "    - output: Output image.\n",
    "    \"\"\"\n",
    "    #df = pd.DataFrame()\n",
    "    # Create a Pandas Excel writer object using XlsxWriter as the engine\n",
    "    #with pd.ExcelWriter(output_excel, engine='xlsxwriter') as writer:\n",
    "        # Convert the DataFrame to an Excel object\n",
    "    output_path = Path(output_path)\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "    for idx, img_pair in enumerate(image_pairs):\n",
    "            # Create a BytesIO object to hold the image data in memory\n",
    "            image_data = BytesIO()\n",
    "\n",
    "            # Generate the matplotlib figure and save it to the BytesIO object\n",
    "            fig = record_pair_image(img_pair)  # Assume the function generates and returns a figure\n",
    "            fig.savefig(output_path / f'{idx}.png', format='png')\n",
    "            plt.close(fig)  # Close the figure to free up memory\n",
    "\n",
    "            # Seek to the beginning of the BytesIO object so it can be read from\n",
    "    print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Output Excel file path\n",
    "output = 'data/linkage_annotations'\n",
    "\n",
    "# Save DataFrame with matplotlib-generated images\n",
    "save_images_for_annoation(image_pairs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heritageweaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
